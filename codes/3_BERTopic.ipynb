{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a589a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import pandas as pd\n",
    "import umap\n",
    "import torch\n",
    "import gc\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d4efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6658a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_bertopic(df, text_col, nr_topics=25):\n",
    "    \"\"\"\n",
    "    Function to implement BERTopic on a dataframe with a specified text column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe containing the text data.\n",
    "        text_col (str): The column name in the dataframe that contains the text to be analyzed.\n",
    "        nr_topics (int): The number of topics to reduce to. Default is 25.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: The trained BERTopic model.\n",
    "    \"\"\"\n",
    "    # Extract the specified text column from the dataframe\n",
    "    comments = df[text_col]\n",
    "    \n",
    "    # Convert all text entries to string format to ensure compatibility\n",
    "    comments = [str(i) for i in comments]\n",
    "    \n",
    "    # Initialize Maximal Marginal Relevance (MMR) for topic diversity\n",
    "    mmr = MaximalMarginalRelevance(diversity=0.9)  # Higher diversity for more varied topics\n",
    "    \n",
    "    # Use MMR as the representation model\n",
    "    representation_model = mmr\n",
    "    \n",
    "    # Initialize the Class-based TF-IDF model to reduce frequent words\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    \n",
    "    # Set up a CountVectorizer with English stopwords\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    # Initialize UMAP (Uniform Manifold Approximation and Projection) for dimensionality reduction\n",
    "    # Set a fixed random state to ensure reproducibility of results\n",
    "    umap_model = umap.UMAP(random_state=42)\n",
    "    \n",
    "    # Configure and initialize the BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        # Representation model to control the diversity of topics\n",
    "        representation_model=representation_model,\n",
    "        \n",
    "        # Vectorizer model for converting text into numerical representations\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        \n",
    "        # Enable verbose output for progress tracking\n",
    "        verbose=True,\n",
    "        \n",
    "        # Class-based TF-IDF model for topic representation\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        \n",
    "        # Number of topics to reduce to\n",
    "        nr_topics=nr_topics,\n",
    "        \n",
    "        # Pass UMAP model for dimensionality reduction with fixed random state\n",
    "        umap_model=umap_model,\n",
    "        \n",
    "        # Calculate probabilities for each topic assignment\n",
    "        calculate_probabilities=True,\n",
    "    )\n",
    "    \n",
    "    # Fit the BERTopic model on the text data and transform to generate topics and probabilities\n",
    "    topics, probs = topic_model.fit_transform(comments)\n",
    "    \n",
    "    # Clear GPU memory to prevent memory leaks\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run garbage collection to free up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return the trained BERTopic model\n",
    "    return topic_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33551941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 10:15:24,940 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147b4cdd60b94e91b5033199ff0d81af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 10:15:30,669 - BERTopic - Embedding - Completed ✓\n",
      "2025-01-13 10:15:30,670 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-01-13 10:16:01,243 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-01-13 10:16:01,247 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-01-13 10:16:02,001 - BERTopic - Cluster - Completed ✓\n",
      "2025-01-13 10:16:02,004 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-01-13 10:16:04,945 - BERTopic - Representation - Completed ✓\n",
      "2025-01-13 10:16:04,949 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-01-13 10:16:06,091 - BERTopic - Topic reduction - Reduced number of topics from 53 to 25\n"
     ]
    }
   ],
   "source": [
    "# read and drop duplicates. \n",
    "df = pd.read_csv('data/depression_SG_reddit.csv.gz', compression='gzip')\n",
    "df['created_utc'] = (pd.to_datetime(df['created_utc'], unit='s'))\n",
    "df=df[df['created_utc'] >= '2015-01-01'].reset_index(drop=True)\n",
    "len(df)\n",
    "df=df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "# implementing BERTopic\n",
    "#df_title=df[df[\"title\"].apply(lambda x: len(x.split())>=5)]\n",
    "topic_model_title=implement_bertopic(df,\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7998eac-0ae9-4f1a-8304-2365271290db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1687</td>\n",
       "      <td>-1_man_help_singaporeans_amp</td>\n",
       "      <td>[man, help, singaporeans, amp, daughter, spore...</td>\n",
       "      <td>[About 12am+ this guy started yelling. Im unsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>0_loneliness_like_sad_destined</td>\n",
       "      <td>[loneliness, like, sad, destined, talk, year, ...</td>\n",
       "      <td>[Helpless as I can no longer stay at my house ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>1_imh_help_affordable_bipolar</td>\n",
       "      <td>[imh, help, affordable, bipolar, seek, counsel...</td>\n",
       "      <td>[I need help on my mental health., Death at Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>2_poly_gpa_secondary_fresh</td>\n",
       "      <td>[poly, gpa, secondary, fresh, options, hiring,...</td>\n",
       "      <td>[Career advice for late twenties early jobber ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>3_internship_sg_just_resignation</td>\n",
       "      <td>[internship, sg, just, resignation, battle, ap...</td>\n",
       "      <td>[just started a internship about a week ago, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>126</td>\n",
       "      <td>4_coronavirus_migrant_measures_pfizer</td>\n",
       "      <td>[coronavirus, migrant, measures, pfizer, wuhan...</td>\n",
       "      <td>[At breaking point: Singapore's migrant worker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>5_attacks_eating_ptsd_antidepressants</td>\n",
       "      <td>[attacks, eating, ptsd, antidepressants, natio...</td>\n",
       "      <td>[Anxiety/Panic attacks, [Serious] People who e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>82</td>\n",
       "      <td>6_pes_nsf_bunk_depression</td>\n",
       "      <td>[pes, nsf, bunk, depression, cofounder, exempt...</td>\n",
       "      <td>[Can you down NS PES from depression?, NOC Co-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>7_heard_shes_assaulted_families</td>\n",
       "      <td>[heard, shes, assaulted, families, december, d...</td>\n",
       "      <td>[My (now ex) boyfriend attempted rape December...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>8_breakup_attachment_unavailable_sensitive</td>\n",
       "      <td>[breakup, attachment, unavailable, sensitive, ...</td>\n",
       "      <td>[How do I (21M) ask my friend out that I previ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9_scam_valuing_transactions_ocbc</td>\n",
       "      <td>[scam, valuing, transactions, ocbc, mule, mone...</td>\n",
       "      <td>[I was tricked into being a money mule, please...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                        Name  \\\n",
       "0      -1   1687                -1_man_help_singaporeans_amp   \n",
       "1       0    241              0_loneliness_like_sad_destined   \n",
       "2       1    229               1_imh_help_affordable_bipolar   \n",
       "3       2    136                  2_poly_gpa_secondary_fresh   \n",
       "4       3    126            3_internship_sg_just_resignation   \n",
       "5       4    126       4_coronavirus_migrant_measures_pfizer   \n",
       "6       5     86       5_attacks_eating_ptsd_antidepressants   \n",
       "7       6     82                   6_pes_nsf_bunk_depression   \n",
       "8       7     36             7_heard_shes_assaulted_families   \n",
       "9       8     24  8_breakup_attachment_unavailable_sensitive   \n",
       "10      9     10            9_scam_valuing_transactions_ocbc   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [man, help, singaporeans, amp, daughter, spore...   \n",
       "1   [loneliness, like, sad, destined, talk, year, ...   \n",
       "2   [imh, help, affordable, bipolar, seek, counsel...   \n",
       "3   [poly, gpa, secondary, fresh, options, hiring,...   \n",
       "4   [internship, sg, just, resignation, battle, ap...   \n",
       "5   [coronavirus, migrant, measures, pfizer, wuhan...   \n",
       "6   [attacks, eating, ptsd, antidepressants, natio...   \n",
       "7   [pes, nsf, bunk, depression, cofounder, exempt...   \n",
       "8   [heard, shes, assaulted, families, december, d...   \n",
       "9   [breakup, attachment, unavailable, sensitive, ...   \n",
       "10  [scam, valuing, transactions, ocbc, mule, mone...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [About 12am+ this guy started yelling. Im unsu...  \n",
       "1   [Helpless as I can no longer stay at my house ...  \n",
       "2   [I need help on my mental health., Death at Ri...  \n",
       "3   [Career advice for late twenties early jobber ...  \n",
       "4   [just started a internship about a week ago, f...  \n",
       "5   [At breaking point: Singapore's migrant worker...  \n",
       "6   [Anxiety/Panic attacks, [Serious] People who e...  \n",
       "7   [Can you down NS PES from depression?, NOC Co-...  \n",
       "8   [My (now ex) boyfriend attempted rape December...  \n",
       "9   [How do I (21M) ask my friend out that I previ...  \n",
       "10  [I was tricked into being a money mule, please...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging similar topics (esp those that pre-trained models will not be familiar with in the Singapore context)\n",
    "# e.g., --> \"PES\" and \"NS\" are related but the pre-trained models are not familiar with those. \n",
    "\n",
    "topic_model_title.merge_topics(list(df[\"title\"]), [[-1,0,3,7,12,14,16,17,23],[21,19,9],[4,20],[18,5],[11,6],[15,2]]) \n",
    "#topic_model_title.merge_topics(list(df[\"title\"]), [[-1,0,3,7,12,14,16,17,23],[21,19,9],[4,20],[11,6],[15,2]]) \n",
    "\n",
    "# get topic ingo\n",
    "topic_models=topic_model_title.get_topic_info()\n",
    "\n",
    "topic_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8aa63a2-f9e5-4d4f-871e-4d62ffa96be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "topic_models.to_csv(\"topic_models_title.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a2ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_clusters=topic_model_title.get_document_info(list(df[\"title\"]))\n",
    "document_clusters=document_clusters.rename(columns={\"Document\":\"title\"})\n",
    "documents=pd.merge(document_clusters,df,how=\"right\",on=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5100152",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.to_csv(\"document_clusters.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d801688-329d-458f-b6b9-a69b6217419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2783"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
